{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbf9554a-a16a-4d96-b16b-97993473eecb",
   "metadata": {},
   "source": [
    "# Batch ETL process with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23131892-e697-4aa8-9437-11d2d7b057d8",
   "metadata": {},
   "source": [
    "The notebook's purpose is to do a batch processing for csv files from the retail company, also is presented the EDA process and store it on postgres cloud service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be240edf-1157-4ad6-b2d8-4798996c3900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /home/alex/.local/lib/python3.12/site-packages (3.5.4)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/alex/.local/lib/python3.12/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark --break-system-packages\n",
    "!pip install -q findspark --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbdb6051-1086-4118-8edc-782bf0ca8d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "from pyspark.sql.functions import col, max, count, when, udf, to_date, sum, greatest, lit, coalesce, upper\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "from pandas import to_datetime, NaT\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time, random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b1d23-bd85-4f4c-90f5-4e2cdcc59ead",
   "metadata": {},
   "source": [
    "In this section we initialize the pyspark session and configure the session to use the jar component to connect with postgresql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2a4a511-cd93-4316-a5f5-41765777a576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/08 17:51:25 WARN Utils: Your hostname, alexrm resolves to a loopback address: 127.0.1.1; using 192.168.100.74 instead (on interface wlp0s20f3)\n",
      "25/04/08 17:51:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/04/08 17:51:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "findspark.init()\n",
    "spark = (SparkSession.builder.master('local[*]')\n",
    "         .appName('Batch processing')\n",
    "         .config('spark.jars', '../../libraries/postgresql-42.7.5.jar')\n",
    "         .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cdf876-68db-4783-9aa9-b9658d5de81a",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940f3b4b-046f-476d-91fd-be8049b7d376",
   "metadata": {},
   "source": [
    "Reading csv files stored in the data folder: `products.csv`, `sales.csv` and `soh.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "385a5de9-ba2a-4f9f-b521-d289d21bad1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gtin: long (nullable = true)\n",
      " |-- productCode: string (nullable = true)\n",
      " |-- size: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products = (\n",
    "    spark.read.format('csv')\n",
    "    .option('header', 'true')\n",
    "    .option('inferSchema', 'true')\n",
    "    .load('../../data/products.csv')\n",
    ")\n",
    "products.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cdd0e6c-9754-4044-8527-27e140c599a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:============================================>              (6 + 2) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sku: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- site_code: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "sales = (\n",
    "    spark.read.format('csv')\n",
    "    .option('header', 'true')\n",
    "    .option('inferSchema', 'true')\n",
    "    .load('../../data/sales*.csv')\n",
    ")\n",
    "sales.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f6d4f98-f3d9-4eb3-ada1-609cea3027f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/08 17:51:44 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- site_code: string (nullable = true)\n",
      " |-- sku: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "soh = (\n",
    "    spark.read.format('csv')\n",
    "    .option('header', 'true')\n",
    "    .option('inferSchema', 'true')\n",
    "    .load('../../data/soh.csv')\n",
    ")\n",
    "soh.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa96bf7-ad19-4430-a00e-e183f2d4ef85",
   "metadata": {},
   "source": [
    "### EDA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a6d3ef-92ae-4dba-a06e-7ea4ebd1bb7d",
   "metadata": {},
   "source": [
    "The section must be visible as EDA analysis and also do some transformations to the data, erasing rows to only include `clean` data into the cloud database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6533f0c-9d2a-4560-aac4-92da46f18ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/08 17:51:47 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+------------------+------+-------------+-----------+\n",
      "|summary|                gtin|productCode|              size| color|        label|   category|\n",
      "+-------+--------------------+-----------+------------------+------+-------------+-----------+\n",
      "|  count|                 451|        451|               451|   451|          451|        451|\n",
      "|   mean|9.780123459034512E12|       NULL|14.813333333333333|  NULL|         NULL|       NULL|\n",
      "| stddev|  1303.3406465853573|       NULL|10.991020085620919|  NULL|         NULL|       NULL|\n",
      "|    min|       9780123456789| A-LINE-044|                10| Beige| A Line Shift|Accessories|\n",
      "|    max|       9780123461288|   YOGA-423|                XS|Yellow|Zip Up Hoodie|       Tops|\n",
      "+-------+--------------------+-----------+------------------+------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd327549-2ec6-4c30-909e-dd69d10cc7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:====================================>                      (5 + 3) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------------+---------+------------------+\n",
      "|summary|       sku|         quantity|site_code|              date|\n",
      "+-------+----------+-----------------+---------+------------------+\n",
      "|  count|   1900538|          1900368|  1900262|           1899906|\n",
      "|   mean|      NULL|6.396592659947968|     NULL|              NULL|\n",
      "| stddev|      NULL|6.464789205766256|     NULL|              NULL|\n",
      "|    min|A-LINE-044|              -15|   AUS000|        01-01-2023|\n",
      "|    max|  YOGA-423|               15|   usa004|September 30, 2024|\n",
      "+-------+----------+-----------------+---------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "sales.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e6e569a-453f-4d95-a865-d7cdd346493d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:==================================================>       (7 + 1) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+-----------------+------------------+\n",
      "|summary|site_code|       sku|         quantity|              date|\n",
      "+-------+---------+----------+-----------------+------------------+\n",
      "|  count|  8619047|   8619977|          8619162|           8620285|\n",
      "|   mean|     NULL|      NULL|574.3629137032116|              NULL|\n",
      "| stddev|     NULL|      NULL|568.1851194776879|              NULL|\n",
      "|    min|   AUS000|A-LINE-044|            -1495|        01-01-2023|\n",
      "|    max|   usa004|  YOGA-423|             1496|September 30, 2024|\n",
      "+-------+---------+----------+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "soh.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3bd181d-5798-43b3-bfd0-c5c0ea56f9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = products.withColumnRenamed('productCode', 'product_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "048f3808-466f-4dcb-b2e6-1b904fa3357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_non_null(df):\n",
    "    \"\"\"\n",
    "    Count non null rows based on each column of the Pyspark DataFrame and print in the console\n",
    "\n",
    "    Parameter: \n",
    "    df (pyspark.DataFrame)\n",
    "    \"\"\"\n",
    "    df.select([count(when(col(c).isNotNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cfa9f17-a075-46f1-83e5-3512c455c800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_outliers(df, column):\n",
    "    quartiles = df.approxQuantile(column, [0.25, 0.75], 0.01)\n",
    "    Q1, Q3 = quartiles[0], quartiles[1]\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound, upper_bound = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "    clean = df.filter((df[column] >= lower_bound) & (df[column] <= upper_bound))\n",
    "    print(f'Qty of rows erased: {((clean.count() / df.count())*100):2f}') \n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68254d97-6051-4e08-8272-f22179e33fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+----+-----+-----+--------+\n",
      "|gtin|product_code|size|color|label|category|\n",
      "+----+------------+----+-----+-----+--------+\n",
      "| 451|         451| 451|  451|  451|     451|\n",
      "+----+------------+----+-----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_non_null(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "228d5b91-0e12-46ee-b30d-1bbf72f9a7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:=============================>                            (4 + 4) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+-------+\n",
      "|    sku|quantity|site_code|   date|\n",
      "+-------+--------+---------+-------+\n",
      "|1900538| 1900368|  1900262|1899906|\n",
      "+-------+--------+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "count_non_null(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "578fab84-f6bb-45d9-9780-ed8d7848d752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:=======>                                                  (1 + 7) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------+-------+\n",
      "|site_code|    sku|quantity|   date|\n",
      "+---------+-------+--------+-------+\n",
      "|  8619047|8619977| 8619162|8620285|\n",
      "+---------+-------+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "count_non_null(soh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "568944a3-0410-4d1d-bb64-044590562380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qty of rows erased: 93.583164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[sku: string, quantity: int, site_code: string, date: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_outliers(sales, 'quantity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9057de90-edd6-4253-b63c-6ab8dc8a2a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:>                                                         (0 + 8) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qty of rows erased: 93.373159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[site_code: string, sku: string, quantity: int, date: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_outliers(soh, 'quantity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af4e307d-49e7-47e9-9e50-f75268cd58f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(date):\n",
    "    \"\"\"\n",
    "    Parse a date which could be in different forms\n",
    "\n",
    "    Parameters:\n",
    "    date (str): A date in a string datatype\n",
    "\n",
    "    Return:\n",
    "    (datetime) Date parsed to the format: yyyy-mm-dd\n",
    "    \"\"\"\n",
    "    date = to_datetime(date, format='mixed', errors='coerce')\n",
    "    if date is NaT:\n",
    "        return None\n",
    "    return date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f719db71-ffa4-4973-8407-797ca9d126eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sample(df, fraction=0.1):\n",
    "    \"\"\"\n",
    "    Select randomly a fraction of rows from a dataframe\n",
    "\n",
    "    Parameters:\n",
    "    df (pyspark.DataFrame): dataframe\n",
    "    fraction (double): The portion of data that should be added \n",
    "    \"\"\"\n",
    "    return df.sample(withReplacement=False, fraction=fraction, seed=time.time()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43e89a34-6eb2-4971-990c-19d9e8123855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_null_values(df):\n",
    "    \"\"\"\n",
    "    Drop null values of a dataframe on any column and also get a statistics before droping the values\n",
    "\n",
    "    Parameters:\n",
    "    df (pyspark.DataFrame)\n",
    "\n",
    "    Return: \n",
    "    (pyspark.DataFrame) return a new dataframe erasing the rows with null values\n",
    "    \"\"\"\n",
    "    df_clean = df.dropna(how='any', subset=None)\n",
    "    initial_shape, clean_shape = df.count(), df_clean.count()\n",
    "    print(f'Percentage of data that will be erased: {(100-clean_shape/initial_shape*100.0):.2f}')\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed197eaf-8d5d-4b5f-ad16-d136a372fad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of data that will be erased: 10.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:=======>                                                  (1 + 7) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of data that will be erased: 10.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[site_code: string, sku: string, quantity: int, date: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_null_values(sales)\n",
    "drop_null_values(soh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7916098-0b06-411b-9b98-53afeb36c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(df, functions):\n",
    "    for f in functions:\n",
    "        df = f(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d663fb-88c1-4ae7-8ad9-657fe3b202d1",
   "metadata": {},
   "source": [
    "defining a UDF (User Definition Function) to apply the function `parse_date` to pyspark.DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83730ae6-4fc0-4133-9195-e5f7bb96d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_date_udf = udf(parse_date, TimestampType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e60ec0-2e6b-4d8b-b1e8-fcf8f13b9808",
   "metadata": {},
   "source": [
    "In this specific case we could see that if we intend to drop null values from dataframes: \n",
    "- 'soh' -> have a small portion of the data length to erase, so it could be safe and add a small variation in the data\n",
    "- 'sales' -> have a small portion but we have to decide if erase or try to fix some values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "839fa515-49f0-40b8-abc7-e70f0d4dfabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of data that will be erased: 7.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:=======>                                                  (1 + 7) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qty of rows erased: 100.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "soh = pipeline(\n",
    "    soh, \n",
    "    [\n",
    "        lambda df: df.fillna({'quantity': 0}), \n",
    "        lambda df: df.select(\n",
    "            col('sku'),\n",
    "            col('date'),\n",
    "            upper(col('site_code')).alias('site_code'),\n",
    "            greatest(col('quantity'), lit(0)).alias('quantity')\n",
    "        ),\n",
    "        drop_null_values,\n",
    "        lambda df: extract_outliers(df, 'quantity'),\n",
    "        lambda df: df.withColumn('date', to_date(parse_date_udf(col('date')))), # parse dates\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f02492-39f2-43fa-bf99-2ca51d26a2d0",
   "metadata": {},
   "source": [
    "In the case of `sales` dataframe, its obvious to think that if there are sales then the quantity must be a value more than 0. Also doing more research on this retail company working with clothes, we see that there are not fractional products, so for this reasons, we see that a sales quantity must be greater than 0 and also quantity is in the natural numbers set. So the conclusion is that quantity must be at least 1 to be a valid record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a8372f7-227a-4f80-979e-192c9735916c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:=============================>                            (4 + 4) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of data that will be erased: 7.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "sales = pipeline(\n",
    "    sales,\n",
    "    [        \n",
    "        lambda df: df.fillna({'quantity': 1}),\n",
    "        lambda df: df.select(\n",
    "            col('sku'),\n",
    "            col('date'),\n",
    "            upper(col('site_code')).alias('site_code'),\n",
    "            greatest(col('quantity'), lit(1)).alias('quantity')\n",
    "        ),\n",
    "        drop_null_values,\n",
    "        #lambda df: extract_outliers(df, 'quantity'),\n",
    "        lambda df: df.withColumn('date', to_date(parse_date_udf(col('date'))))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f20eaa65-e911-42fa-bfe5-9ca5d4f88a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pipeline(\n",
    "    products,\n",
    "    [\n",
    "        lambda df: df.withColumnRenamed('productCode', 'product_code'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62deb335-50a5-4435-a5a5-4cdfab23da03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+--------+\n",
      "|    sku|   date|site_code|quantity|\n",
      "+-------+-------+---------+--------+\n",
      "|1802336|1802336|  1802336| 1802336|\n",
      "+-------+-------+---------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "1802336"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_non_null(sales)\n",
    "sample(sales, 0.01)\n",
    "#sales.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "543eb5fe-7bc4-4c9e-8199-92a7d854088e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 8) / 8]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alex/.local/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alex/.local/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcount_non_null\u001b[49m\u001b[43m(\u001b[49m\u001b[43msoh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m sample(soh, \u001b[38;5;241m0.0001\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m, in \u001b[0;36mcount_non_null\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_non_null\u001b[39m(df):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Count non null rows based on each column of the Pyspark DataFrame and print in the console\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    Parameter: \u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    df (pyspark.DataFrame)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misNotNull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pyspark/sql/dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \n\u001b[1;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pyspark/sql/dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    962\u001b[0m     )\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count_non_null(soh)\n",
    "sample(soh, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7622e-903b-407b-8b85-700c49641c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_non_null(products)\n",
    "sample(products, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a66bfa-572d-4905-b6e4-ad1e82e48413",
   "metadata": {},
   "outputs": [],
   "source": [
    "soh = (\n",
    "    soh\n",
    "    .withColumn('date', to_date(parse_date_udf(col('date'))))\n",
    "    .groupby('site_code', 'sku', 'date')\n",
    "    .agg(sum('quantity').alias('quantity'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07017b0a-86ca-4253-a91e-b6d8f670cb8f",
   "metadata": {},
   "source": [
    "### Saving in cloud storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165b50b2-9044-46a7-9438-c3309ac892c1",
   "metadata": {},
   "source": [
    "In this last section the cleaned data is stored into a cloud database based on postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d985e7-fa1f-4144-858a-44275f7f552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_url = os.getenv('DB_JDBC_URL')\n",
    "properties = {\n",
    "    \"user\": os.getenv('DB_USER'),\n",
    "    \"password\": os.getenv('DB_PASSWORD'),\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d241c1-f31f-4f4c-8fb8-88106306f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "products.write.jdbc(url=jdbc_url, table='products', mode='append', properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fe2100-bcd6-4e6b-9a6c-904957d6d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.write.jdbc(url=jdbc_url, table='sales', mode='append', properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988df380-dff9-4d60-8c12-e669dbec7ae3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soh.write.jdbc(url=jdbc_url, table='soh', mode='append', properties=properties)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
