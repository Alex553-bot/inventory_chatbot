{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03baaab7-1620-4740-ae97-ea7c45d07aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: joblib in /usr/lib/python3/dist-packages (1.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install joblib --break-system-packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d38d53-8a80-4286-bd19-051c730601f9",
   "metadata": {},
   "source": [
    "# Training an deep learning model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd60dd4-4eb1-4d1c-82f4-c35013189f88",
   "metadata": {},
   "source": [
    "The notebook's purpose is to build a model using tensorflow and LSTM layers to predict the future sales based on the data in the datawarehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97d4c7d7-e3c1-48ff-aa0d-9a788704a3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 06:00:50.494637: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-01 06:00:51.525016: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-01 06:00:52.100179: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743501652.703530    6297 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743501652.956446    6297 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-01 06:00:54.247474: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Concatenate, Bidirectional, LSTM, Dense\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../libraries')\n",
    "from database import get_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc58e6d0-dc43-40a0-ad4b-209fa4d83afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(skus, site_codes):\n",
    "    \"\"\"Loads the data from cloud storage\"\"\"\n",
    "    formatted_skus = ', '.join(f\"'{sku}'\" for sku in skus)\n",
    "\n",
    "    # Format site_codes for SQL IN clause\n",
    "    formatted_site_codes = ', '.join(f\"'{site_code}'\" for site_code in site_codes)\n",
    "    query = f\"\"\"\n",
    "SELECT \n",
    "    s.sku, \n",
    "    SUM(s.quantity) as quantity, \n",
    "    s.date, \n",
    "    s.site_code, \n",
    "    p.category\n",
    "FROM \n",
    "    sales s\n",
    "LEFT JOIN \n",
    "    products p on p.product_code = s.sku\n",
    "WHERE \n",
    "    s.sku IN ({formatted_skus}) AND s.site_code IN ({formatted_site_codes})\n",
    "GROUP BY \n",
    "    s.date, s.sku, p.category, s.site_code\n",
    "ORDER BY \n",
    "    s.site_code, s.sku, s.date;\n",
    "    \"\"\"\n",
    "    return get_results(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a16ba8f8-d64f-4b6b-993d-adf6098c828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skus(): \n",
    "    return get_results('''\n",
    "SELECT \n",
    "    product_code AS sku\n",
    "FROM \n",
    "    products\n",
    "    ''').sku.values\n",
    "def get_site_codes(): \n",
    "    return get_results('''\n",
    "SELECT \n",
    "    DISTINCT site_code\n",
    "FROM \n",
    "    sales\n",
    "    ''').site_code.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3c1fccb-7a4c-4610-9182-8fd4cc3e1dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bidirectional_lstm_model(num_numerical_features, num_unique_site_codes, num_unique_skus, num_unique_categories, num_unique_seasons):\n",
    "    numerical_input = Input(shape=(None, num_numerical_features), name='numerical_input')\n",
    "    site_code_input = Input(shape=(None,), dtype='int32', name='site_code_input')\n",
    "    sku_input = Input(shape=(None,), dtype='int32', name='sku_input')\n",
    "    category_input = Input(shape=(None,), dtype='int32', name='category_input')\n",
    "    season_input = Input(shape=(None,), dtype='int32', name='season_input')\n",
    "\n",
    "    site_code_embedding = Embedding(input_dim=num_unique_site_codes, output_dim=10)(site_code_input)\n",
    "    sku_embedding = Embedding(input_dim=num_unique_skus, output_dim=20)(sku_input)\n",
    "    category_embedding = Embedding(input_dim=num_unique_categories, output_dim=8)(category_input)\n",
    "    season_embedding = Embedding(input_dim=num_unique_seasons, output_dim=5)(season_input)\n",
    "\n",
    "    concatenated_inputs = Concatenate(axis=-1)([numerical_input, site_code_embedding, sku_embedding, category_embedding, season_embedding])\n",
    "    lstm = Bidirectional(LSTM(64, return_sequences=True))(concatenated_inputs)\n",
    "    lstm = Bidirectional(LSTM(32))(lstm)\n",
    "    output = Dense(1)(lstm)\n",
    "\n",
    "    model = Model(inputs=[numerical_input, site_code_input, sku_input, category_input, season_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddaf1ff0-74b6-4522-af53-934c1c2c5daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_encoders():\n",
    "    skus, site_codes = get_skus(), get_site_codes()\n",
    "    all_site_codes = site_codes\n",
    "    all_skus = skus\n",
    "    all_categories = get_results('''\n",
    "SELECT \n",
    "    DISTINCT category \n",
    "FROM \n",
    "    products\n",
    "    ''').category.values.astype(str)\n",
    "    all_seasons = [i for i in range(0, 4)]\n",
    "\n",
    "    site_code_encoder = LabelEncoder()\n",
    "    site_code_encoder.fit(all_site_codes)\n",
    "    sku_encoder = LabelEncoder()\n",
    "    sku_encoder.fit(all_skus)\n",
    "    category_encoder = LabelEncoder()\n",
    "    category_encoder.fit(all_categories)\n",
    "    season_encoder = LabelEncoder()\n",
    "    season_encoder.fit(all_seasons)\n",
    "\n",
    "    encoders = {\n",
    "        \"site_code\": site_code_encoder,\n",
    "        \"sku\": sku_encoder,\n",
    "        \"category\": category_encoder,\n",
    "        \"season\": season_encoder,\n",
    "    }\n",
    "\n",
    "    return encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bc13248-b5b8-4eae-9d28-4b704a6f083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df_batch, encoders):\n",
    "    df_batch['date'] = pd.to_datetime(df_batch['date'])\n",
    "    df_batch['day_of_week'] = df_batch['date'].dt.dayofweek\n",
    "    df_batch['month'] = df_batch['date'].dt.month\n",
    "\n",
    "    df_batch['site_code'] = encoders['site_code'].transform(df_batch['site_code'])\n",
    "    df_batch['sku'] = encoders['sku'].transform(df_batch['sku'])\n",
    "    df_batch['category'] = encoders['category'].transform(df_batch['category'])\n",
    "    df_batch['season'] = df_batch['date'].apply(lambda x: (x.month - 1) // 3)\n",
    "    df_batch['season'] = encoders['season'].transform(df_batch['season'])\n",
    "\n",
    "    numerical_features = ['quantity', 'day_of_week', 'month']\n",
    "    scaler = StandardScaler()\n",
    "    df_batch[numerical_features] = scaler.fit_transform(df_batch[numerical_features])\n",
    "\n",
    "    return df_batch, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf2f2fc8-15ef-4203-a23c-7cabc05d2ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(df_batch, sequence_length):\n",
    "    numerical_data = df_batch[['quantity', 'day_of_week', 'month']].values\n",
    "    site_code_data = df_batch['site_code'].values\n",
    "    sku_data = df_batch['sku'].values\n",
    "    category_data = df_batch['category'].values\n",
    "    season_data = df_batch['season'].values\n",
    "    target_data = df_batch['quantity'].values\n",
    "\n",
    "    numerical_batches = []\n",
    "    site_code_batches = []\n",
    "    sku_batches = []\n",
    "    category_batches = []\n",
    "    season_batches = []\n",
    "    target_batches = []\n",
    "\n",
    "    for i in range(0, len(df_batch) - sequence_length):\n",
    "        numerical_batches.append(numerical_data[i:i + sequence_length])\n",
    "        site_code_batches.append(site_code_data[i:i + sequence_length])\n",
    "        sku_batches.append(sku_data[i:i + sequence_length])\n",
    "        category_batches.append(category_data[i:i + sequence_length])\n",
    "        season_batches.append(season_data[i:i + sequence_length])\n",
    "        target_batches.append(target_data[i + sequence_length])\n",
    "\n",
    "    numerical_batches = pad_sequences(numerical_batches, dtype='float32')\n",
    "    site_code_batches = pad_sequences(site_code_batches, dtype='int32')\n",
    "    sku_batches = pad_sequences(sku_batches, dtype='int32')\n",
    "    category_batches = pad_sequences(category_batches, dtype='int32')\n",
    "    season_batches = pad_sequences(season_batches, dtype='int32')\n",
    "    target_batches = np.array(target_batches)\n",
    "\n",
    "    return numerical_batches, site_code_batches, sku_batches, category_batches, season_batches, target_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e16ddb7-6f0a-47b5-90b8-63977a60aaf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_batches(sequence_length=10, batch_size=32, model_save_path=\"model_stmp.h5\", encoders_save_path=\"encoders.pkl\"):\n",
    "    encoders = create_label_encoders()\n",
    "\n",
    "    num_numerical_features = 3\n",
    "    num_unique_site_codes = len(encoders['site_code'].classes_)\n",
    "    num_unique_skus = len(encoders['sku'].classes_)\n",
    "    num_unique_categories = len(encoders['category'].classes_)\n",
    "    num_unique_seasons = len(encoders['season'].classes_)\n",
    "\n",
    "    model = build_bidirectional_lstm_model(num_numerical_features, num_unique_site_codes, num_unique_skus, num_unique_categories, num_unique_seasons)\n",
    "\n",
    "    skus, site_codes = get_skus(), get_site_codes()\n",
    "\n",
    "    for site_code in site_codes:\n",
    "        batch_df = load_data(skus, [site_code])\n",
    "        if batch_df.shape[0] == 0: continue\n",
    "        batch_df, scaler = preprocess_data(batch_df, encoders)\n",
    "        numerical_batches, site_code_batches, sku_batches, category_batches, season_batches, target_batches = create_batch(batch_df, sequence_length)\n",
    "        model.fit(\n",
    "            [numerical_batches, site_code_batches, sku_batches, category_batches, season_batches],\n",
    "            target_batches,\n",
    "            epochs=5,\n",
    "            batch_size=batch_size,\n",
    "            verbose=1\n",
    "        )\n",
    "        print(f'Model trained with SITE CODE: {site_code}') \n",
    "\n",
    "    model.save(model_save_path)\n",
    "    encoders[\"scaler\"] = scaler\n",
    "    joblib.dump(encoders, encoders_save_path)\n",
    "\n",
    "    return model, encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c617ed8c-6601-4372-b519-a9a43328838d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.9967\n",
      "Epoch 2/5\n",
      "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 1.0049\n",
      "Epoch 3/5\n",
      "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.9997\n",
      "Epoch 4/5\n",
      "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 1.0161\n",
      "Epoch 5/5\n",
      "\u001b[1m939/939\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9899\n",
      "Model trained with SITE CODE: AUS000\n",
      "Epoch 1/5\n",
      "\u001b[1m958/958\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 1.0148\n",
      "Epoch 2/5\n",
      "\u001b[1m958/958\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9972\n",
      "Epoch 3/5\n",
      "\u001b[1m958/958\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9826\n",
      "Epoch 4/5\n",
      "\u001b[1m958/958\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.9832 \n",
      "Epoch 5/5\n",
      "\u001b[1m958/958\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - loss: 0.9731\n",
      "Model trained with SITE CODE: AUS001\n",
      "Epoch 1/5\n",
      "\u001b[1m964/964\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 1.0081\n",
      "Epoch 2/5\n",
      "\u001b[1m964/964\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0013\n",
      "Epoch 3/5\n",
      "\u001b[1m964/964\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0105\n",
      "Epoch 4/5\n",
      "\u001b[1m964/964\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9912\n",
      "Epoch 5/5\n",
      "\u001b[1m964/964\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9699\n",
      "Model trained with SITE CODE: AUS002\n",
      "Epoch 1/5\n",
      "\u001b[1m931/931\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0083\n",
      "Epoch 2/5\n",
      "\u001b[1m931/931\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9886\n",
      "Epoch 3/5\n",
      "\u001b[1m931/931\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9743\n",
      "Epoch 4/5\n",
      "\u001b[1m931/931\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9880\n",
      "Epoch 5/5\n",
      "\u001b[1m931/931\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9734\n",
      "Model trained with SITE CODE: AUS003\n",
      "Epoch 1/5\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0142\n",
      "Epoch 2/5\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9917\n",
      "Epoch 3/5\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - loss: 0.9809\n",
      "Epoch 4/5\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9742\n",
      "Epoch 5/5\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9600\n",
      "Model trained with SITE CODE: AUS004\n",
      "Epoch 1/5\n",
      "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9967\n",
      "Epoch 2/5\n",
      "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9857\n",
      "Epoch 3/5\n",
      "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9852\n",
      "Epoch 4/5\n",
      "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9836\n",
      "Epoch 5/5\n",
      "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9594\n",
      "Model trained with SITE CODE: BRA000\n",
      "Epoch 1/5\n",
      "\u001b[1m903/903\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0218\n",
      "Epoch 2/5\n",
      "\u001b[1m903/903\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9842\n",
      "Epoch 3/5\n",
      "\u001b[1m903/903\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9981\n",
      "Epoch 4/5\n",
      "\u001b[1m903/903\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9562\n",
      "Epoch 5/5\n",
      "\u001b[1m903/903\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.9577\n",
      "Model trained with SITE CODE: BRA001\n",
      "Epoch 1/5\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 1.0110\n",
      "Epoch 2/5\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0005\n",
      "Epoch 3/5\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9724\n",
      "Epoch 4/5\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9614\n",
      "Epoch 5/5\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9302\n",
      "Model trained with SITE CODE: BRA002\n",
      "Epoch 1/5\n",
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0225\n",
      "Epoch 2/5\n",
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0122\n",
      "Epoch 3/5\n",
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9806\n",
      "Epoch 4/5\n",
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9443\n",
      "Epoch 5/5\n",
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9214\n",
      "Model trained with SITE CODE: BRA003\n",
      "Epoch 1/5\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0257\n",
      "Epoch 2/5\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9810\n",
      "Epoch 3/5\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9747\n",
      "Epoch 4/5\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9632\n",
      "Epoch 5/5\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9270\n",
      "Model trained with SITE CODE: BRA004\n",
      "Epoch 1/5\n",
      "\u001b[1m958/958\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0166\n",
      "Epoch 2/5\n",
      "\u001b[1m958/958\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9702\n",
      "Epoch 3/5\n",
      "\u001b[1m958/958\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9606 \n",
      "Epoch 4/5\n",
      "\u001b[1m958/958\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9453\n",
      "Epoch 5/5\n",
      "\u001b[1m958/958\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9073\n",
      "Model trained with SITE CODE: CAN000\n",
      "Epoch 1/5\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0189\n",
      "Epoch 2/5\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9922\n",
      "Epoch 3/5\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9803\n",
      "Epoch 4/5\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9464\n",
      "Epoch 5/5\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9182\n",
      "Model trained with SITE CODE: CAN001\n",
      "Epoch 1/5\n",
      "\u001b[1m953/953\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0274\n",
      "Epoch 2/5\n",
      "\u001b[1m953/953\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9926\n",
      "Epoch 3/5\n",
      "\u001b[1m953/953\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9874\n",
      "Epoch 4/5\n",
      "\u001b[1m953/953\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9652\n",
      "Epoch 5/5\n",
      "\u001b[1m953/953\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9212\n",
      "Model trained with SITE CODE: CAN002\n",
      "Epoch 1/5\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0250\n",
      "Epoch 2/5\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9792\n",
      "Epoch 3/5\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9803\n",
      "Epoch 4/5\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.9423\n",
      "Epoch 5/5\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.9130\n",
      "Model trained with SITE CODE: CAN003\n",
      "Epoch 1/5\n",
      "\u001b[1m943/943\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 1.0361\n",
      "Epoch 2/5\n",
      "\u001b[1m943/943\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.9812\n",
      "Epoch 3/5\n",
      "\u001b[1m943/943\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.9798\n",
      "Epoch 4/5\n",
      "\u001b[1m943/943\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.9570\n",
      "Epoch 5/5\n",
      "\u001b[1m943/943\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9371\n",
      "Model trained with SITE CODE: CAN004\n",
      "Epoch 1/5\n",
      "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 1.0339\n",
      "Epoch 2/5\n",
      "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.9875\n",
      "Epoch 3/5\n",
      "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.9848\n",
      "Epoch 4/5\n",
      "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.9404\n",
      "Epoch 5/5\n",
      "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.9039\n",
      "Model trained with SITE CODE: FRA000\n",
      "Epoch 1/5\n",
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 1.0374\n",
      "Epoch 2/5\n",
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.9830\n",
      "Epoch 3/5\n",
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.9772\n",
      "Epoch 4/5\n",
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9536\n",
      "Epoch 5/5\n",
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.8964\n",
      "Model trained with SITE CODE: FRA001\n",
      "Epoch 1/5\n",
      "\u001b[1m966/966\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 1.0496\n",
      "Epoch 2/5\n",
      "\u001b[1m966/966\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9969\n",
      "Epoch 3/5\n",
      "\u001b[1m966/966\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.9610\n",
      "Epoch 4/5\n",
      "\u001b[1m966/966\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.9609\n",
      "Epoch 5/5\n",
      "\u001b[1m966/966\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.8922\n",
      "Model trained with SITE CODE: FRA002\n",
      "Epoch 1/5\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0204\n",
      "Epoch 2/5\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9808\n",
      "Epoch 3/5\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9757\n",
      "Epoch 4/5\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9507\n",
      "Epoch 5/5\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9340\n",
      "Model trained with SITE CODE: FRA003\n",
      "Epoch 1/5\n",
      "\u001b[1m942/942\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 1.0215\n",
      "Epoch 2/5\n",
      "\u001b[1m942/942\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.9927\n",
      "Epoch 3/5\n",
      "\u001b[1m942/942\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9660\n",
      "Epoch 4/5\n",
      "\u001b[1m942/942\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9448\n",
      "Epoch 5/5\n",
      "\u001b[1m942/942\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9314\n",
      "Model trained with SITE CODE: FRA004\n",
      "Epoch 1/5\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0294\n",
      "Epoch 2/5\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0003\n",
      "Epoch 3/5\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9756\n",
      "Epoch 4/5\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9428\n",
      "Epoch 5/5\n",
      "\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9086\n",
      "Model trained with SITE CODE: GER000\n",
      "Epoch 1/5\n",
      "\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0371\n",
      "Epoch 2/5\n",
      "\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9926\n",
      "Epoch 3/5\n",
      "\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9779\n",
      "Epoch 4/5\n",
      "\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.9538\n",
      "Epoch 5/5\n",
      "\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 0.9275 \n",
      "Model trained with SITE CODE: GER001\n",
      "Epoch 1/5\n",
      "\u001b[1m912/912\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - loss: 1.0155  \n",
      "Epoch 2/5\n",
      "\u001b[1m912/912\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9929\n",
      "Epoch 3/5\n",
      "\u001b[1m912/912\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9702\n",
      "Epoch 4/5\n",
      "\u001b[1m912/912\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9540\n",
      "Epoch 5/5\n",
      "\u001b[1m912/912\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9374\n",
      "Model trained with SITE CODE: GER002\n",
      "Epoch 1/5\n",
      "\u001b[1m957/957\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 1.0172\n",
      "Epoch 2/5\n",
      "\u001b[1m957/957\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.9925\n",
      "Epoch 3/5\n",
      "\u001b[1m957/957\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9928\n",
      "Epoch 4/5\n",
      "\u001b[1m957/957\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9452\n",
      "Epoch 5/5\n",
      "\u001b[1m957/957\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9113\n",
      "Model trained with SITE CODE: GER003\n",
      "Epoch 1/5\n",
      "\u001b[1m950/950\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0141\n",
      "Epoch 2/5\n",
      "\u001b[1m950/950\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 1.0088\n",
      "Epoch 3/5\n",
      "\u001b[1m950/950\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9872\n",
      "Epoch 4/5\n",
      "\u001b[1m950/950\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9714\n",
      "Epoch 5/5\n",
      "\u001b[1m950/950\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9479\n",
      "Model trained with SITE CODE: GER004\n",
      "Epoch 1/5\n",
      "\u001b[1m958/958\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 1.0259\n",
      "Epoch 2/5\n",
      "\u001b[1m958/958\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9949\n",
      "Epoch 3/5\n",
      "\u001b[1m958/958\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9666\n",
      "Epoch 4/5\n",
      "\u001b[1m958/958\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9612\n",
      "Epoch 5/5\n",
      "\u001b[1m958/958\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9288\n",
      "Model trained with SITE CODE: IND000\n",
      "Epoch 1/5\n",
      "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - loss: 1.0173\n",
      "Epoch 2/5\n",
      "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9894\n",
      "Epoch 3/5\n",
      "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9897\n",
      "Epoch 4/5\n",
      "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9616\n",
      "Epoch 5/5\n",
      "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9367\n",
      "Model trained with SITE CODE: IND001\n",
      "Epoch 1/5\n",
      "\u001b[1m969/969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0214\n",
      "Epoch 2/5\n",
      "\u001b[1m969/969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 1.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m969/969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.9775\n",
      "Epoch 4/5\n",
      "\u001b[1m969/969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.9680\n",
      "Epoch 5/5\n",
      "\u001b[1m969/969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.9437\n",
      "Model trained with SITE CODE: IND002\n",
      "Epoch 1/5\n",
      "\u001b[1m907/907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 1.0386\n",
      "Epoch 2/5\n",
      "\u001b[1m907/907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9993\n",
      "Epoch 3/5\n",
      "\u001b[1m907/907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - loss: 0.9820\n",
      "Epoch 4/5\n",
      "\u001b[1m907/907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9569\n",
      "Epoch 5/5\n",
      "\u001b[1m907/907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 0.9377 \n",
      "Model trained with SITE CODE: IND003\n",
      "Epoch 1/5\n",
      "\u001b[1m961/961\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - loss: 1.0091\n",
      "Epoch 2/5\n",
      "\u001b[1m961/961\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - loss: 1.0004\n",
      "Epoch 3/5\n",
      "\u001b[1m961/961\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9690\n",
      "Epoch 4/5\n",
      "\u001b[1m961/961\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9636\n",
      "Epoch 5/5\n",
      "\u001b[1m961/961\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9085\n",
      "Model trained with SITE CODE: IND004\n",
      "Epoch 1/5\n",
      "\u001b[1m943/943\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 1.0289\n",
      "Epoch 2/5\n",
      "\u001b[1m943/943\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - loss: 1.0011\n",
      "Epoch 3/5\n",
      "\u001b[1m943/943\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9857\n",
      "Epoch 4/5\n",
      "\u001b[1m943/943\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9666\n",
      "Epoch 5/5\n",
      "\u001b[1m943/943\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9345\n",
      "Model trained with SITE CODE: JAP000\n",
      "Epoch 1/5\n",
      "\u001b[1m963/963\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 1.0337\n",
      "Epoch 2/5\n",
      "\u001b[1m963/963\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9932\n",
      "Epoch 3/5\n",
      "\u001b[1m963/963\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9741\n",
      "Epoch 4/5\n",
      "\u001b[1m963/963\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9554\n",
      "Epoch 5/5\n",
      "\u001b[1m963/963\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9162\n",
      "Model trained with SITE CODE: JAP001\n",
      "Epoch 1/5\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0282\n",
      "Epoch 2/5\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0077\n",
      "Epoch 3/5\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9818\n",
      "Epoch 4/5\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9754\n",
      "Epoch 5/5\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9509\n",
      "Model trained with SITE CODE: JAP002\n",
      "Epoch 1/5\n",
      "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 1.0249\n",
      "Epoch 2/5\n",
      "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.9845\n",
      "Epoch 3/5\n",
      "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9825\n",
      "Epoch 4/5\n",
      "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9694\n",
      "Epoch 5/5\n",
      "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9397\n",
      "Model trained with SITE CODE: JAP003\n",
      "Epoch 1/5\n",
      "\u001b[1m928/928\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0195\n",
      "Epoch 2/5\n",
      "\u001b[1m928/928\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.9964\n",
      "Epoch 3/5\n",
      "\u001b[1m928/928\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9806\n",
      "Epoch 4/5\n",
      "\u001b[1m928/928\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9780\n",
      "Epoch 5/5\n",
      "\u001b[1m928/928\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9408\n",
      "Model trained with SITE CODE: JAP004\n",
      "Epoch 1/5\n",
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 1.0195\n",
      "Epoch 2/5\n",
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.9970\n",
      "Epoch 3/5\n",
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9766\n",
      "Epoch 4/5\n",
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9472\n",
      "Epoch 5/5\n",
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9265\n",
      "Model trained with SITE CODE: MEX000\n",
      "Epoch 1/5\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0342\n",
      "Epoch 2/5\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0038\n",
      "Epoch 3/5\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9856\n",
      "Epoch 4/5\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9574\n",
      "Epoch 5/5\n",
      "\u001b[1m968/968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9431\n",
      "Model trained with SITE CODE: MEX001\n",
      "Epoch 1/5\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - loss: 1.0209\n",
      "Epoch 2/5\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - loss: 0.9847\n",
      "Epoch 3/5\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9840\n",
      "Epoch 4/5\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9842\n",
      "Epoch 5/5\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9461\n",
      "Model trained with SITE CODE: MEX002\n",
      "Epoch 1/5\n",
      "\u001b[1m961/961\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0142\n",
      "Epoch 2/5\n",
      "\u001b[1m961/961\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9807\n",
      "Epoch 3/5\n",
      "\u001b[1m961/961\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9808\n",
      "Epoch 4/5\n",
      "\u001b[1m961/961\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9648\n",
      "Epoch 5/5\n",
      "\u001b[1m961/961\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9347\n",
      "Model trained with SITE CODE: MEX003\n",
      "Epoch 1/5\n",
      "\u001b[1m965/965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 1.0243\n",
      "Epoch 2/5\n",
      "\u001b[1m965/965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.9826\n",
      "Epoch 3/5\n",
      "\u001b[1m965/965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.9798\n",
      "Epoch 4/5\n",
      "\u001b[1m965/965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9631\n",
      "Epoch 5/5\n",
      "\u001b[1m965/965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9306\n",
      "Model trained with SITE CODE: MEX004\n",
      "Epoch 1/5\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - loss: 1.0302\n",
      "Epoch 2/5\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9908\n",
      "Epoch 3/5\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 1.0085\n",
      "Epoch 4/5\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9795\n",
      "Epoch 5/5\n",
      "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9515\n",
      "Model trained with SITE CODE: UK000\n",
      "Epoch 1/5\n",
      "\u001b[1m1172/1172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 1.0332\n",
      "Epoch 2/5\n",
      "\u001b[1m1172/1172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 0.9949\n",
      "Epoch 3/5\n",
      "\u001b[1m1172/1172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.9781  \n",
      "Epoch 4/5\n",
      "\u001b[1m1172/1172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.9681\n",
      "Epoch 5/5\n",
      "\u001b[1m1172/1172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 0.9437\n",
      "Model trained with SITE CODE: UK001\n",
      "Epoch 1/5\n",
      "\u001b[1m1118/1118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - loss: 1.0141\n",
      "Epoch 2/5\n",
      "\u001b[1m1118/1118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 1.0000\n",
      "Epoch 3/5\n",
      "\u001b[1m1118/1118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 0.9847\n",
      "Epoch 4/5\n",
      "\u001b[1m1118/1118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 0.9870\n",
      "Epoch 5/5\n",
      "\u001b[1m1118/1118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 0.9534\n",
      "Model trained with SITE CODE: UK002\n",
      "Epoch 1/5\n",
      "\u001b[1m1118/1118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 1.0060\n",
      "Epoch 2/5\n",
      "\u001b[1m1118/1118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 1.0048\n",
      "Epoch 3/5\n",
      "\u001b[1m1118/1118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 0.9828\n",
      "Epoch 4/5\n",
      "\u001b[1m1118/1118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.9672\n",
      "Epoch 5/5\n",
      "\u001b[1m1118/1118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 0.9397\n",
      "Model trained with SITE CODE: UK003\n",
      "Epoch 1/5\n",
      "\u001b[1m1118/1118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 1.0130\n",
      "Epoch 2/5\n",
      "\u001b[1m1118/1118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 1.0009\n",
      "Epoch 3/5\n",
      "\u001b[1m1118/1118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 0.9688\n",
      "Epoch 4/5\n",
      "\u001b[1m1118/1118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.9590\n",
      "Epoch 5/5\n",
      "\u001b[1m1118/1118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 0.9405\n",
      "Model trained with SITE CODE: UK004\n",
      "Epoch 1/5\n",
      "\u001b[1m1410/1410\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 1.0033\n",
      "Epoch 2/5\n",
      "\u001b[1m1410/1410\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.9943\n",
      "Epoch 3/5\n",
      "\u001b[1m1410/1410\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.9791\n",
      "Epoch 4/5\n",
      "\u001b[1m1410/1410\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.9808\n",
      "Epoch 5/5\n",
      "\u001b[1m1410/1410\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.9475\n",
      "Model trained with SITE CODE: USA000\n",
      "Epoch 1/5\n",
      "\u001b[1m1374/1374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 1.0186\n",
      "Epoch 2/5\n",
      "\u001b[1m1374/1374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.9942\n",
      "Epoch 3/5\n",
      "\u001b[1m1374/1374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.9738\n",
      "Epoch 4/5\n",
      "\u001b[1m1374/1374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.9721\n",
      "Epoch 5/5\n",
      "\u001b[1m1374/1374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.9533\n",
      "Model trained with SITE CODE: USA001\n",
      "Epoch 1/5\n",
      "\u001b[1m1370/1370\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 1.0177\n",
      "Epoch 2/5\n",
      "\u001b[1m1370/1370\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.9841\n",
      "Epoch 3/5\n",
      "\u001b[1m1370/1370\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.9768\n",
      "Epoch 4/5\n",
      "\u001b[1m1370/1370\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.9762\n",
      "Epoch 5/5\n",
      "\u001b[1m1370/1370\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.9462\n",
      "Model trained with SITE CODE: USA002\n",
      "Epoch 1/5\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 1.0125\n",
      "Epoch 2/5\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.9989\n",
      "Epoch 3/5\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 1.0014\n",
      "Epoch 4/5\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.9677\n",
      "Epoch 5/5\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.9399\n",
      "Model trained with SITE CODE: USA003\n",
      "Epoch 1/5\n",
      "\u001b[1m1382/1382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 1.0300\n",
      "Epoch 2/5\n",
      "\u001b[1m1382/1382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.9988\n",
      "Epoch 3/5\n",
      "\u001b[1m1382/1382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.9839\n",
      "Epoch 4/5\n",
      "\u001b[1m1382/1382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.9676\n",
      "Epoch 5/5\n",
      "\u001b[1m1382/1382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.9526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained with SITE CODE: USA004\n"
     ]
    }
   ],
   "source": [
    "model, mappings = train_batches()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
